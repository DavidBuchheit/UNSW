{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Need to hardcode the location so Jupyter can find the files\n",
    "train_set = 'C:/Users/David/Desktop/Tensorflow/UNSW Data/UNSW_NB15_training-set.csv'\n",
    "test_set = 'C:/Users\\David/Desktop/Tensorflow/UNSW Data/UNSW_NB15_testing-set.csv'\n",
    "\n",
    "training = pd.read_csv(train_set, index_col='id')\n",
    "test = pd.read_csv(test_set, index_col='id')\n",
    "\n",
    "data = pd.concat([training, test]) # Just merging the sets for now as training doesn't match test columns when ONEHOT\n",
    "                                   # And this is just easier for now when testing stuff\n",
    "data_cols = data.columns.drop('label')\n",
    "data = data[data_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "         dur proto service state  spkts  dpkts  sbytes  dbytes       rate  \\\n",
      "id                                                                          \n",
      "1   0.121478   tcp       -   FIN      6      4     258     172  74.087490   \n",
      "2   0.649902   tcp       -   FIN     14     38     734   42014  78.473372   \n",
      "3   1.623129   tcp       -   FIN      8     16     364   13186  14.170161   \n",
      "4   1.681642   tcp     ftp   FIN     12     12     628     770  13.677108   \n",
      "5   0.449454   tcp       -   FIN     10      6     534     268  33.373826   \n",
      "\n",
      "    sttl  ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  \\\n",
      "id        ...                                                               \n",
      "1    252  ...                 1               1             0           0   \n",
      "2     62  ...                 1               2             0           0   \n",
      "3     62  ...                 1               3             0           0   \n",
      "4     62  ...                 1               3             1           1   \n",
      "5    254  ...                 1              40             0           0   \n",
      "\n",
      "    ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  attack_cat  \\\n",
      "id                                                                          \n",
      "1                  0           1           1                0      Normal   \n",
      "2                  0           1           6                0      Normal   \n",
      "3                  0           2           6                0      Normal   \n",
      "4                  0           2           1                0      Normal   \n",
      "5                  0           2          39                0      Normal   \n",
      "\n",
      "    label  \n",
      "id         \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "5       0  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "##Looking for attack_cat\n",
    "CATEGORICAL_COLUMNS = ['proto', 'service', 'state']\n",
    "NUMERIC_COLUMNS = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload',\n",
    "                   'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb',\n",
    "                   'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
    "                   'response_body_len', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login',\n",
    "                   'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm',\n",
    "                   'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_sm_ips_ports']\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    encode_text_dummy(data, col)\n",
    "\n",
    "for col in NUMERIC_COLUMNS:\n",
    "    encode_numeric_zscore(data, col)\n",
    "\n",
    "training.dropna(inplace=True, axis=1)\n",
    "test.dropna(inplace=True, axis=1)\n",
    "print(training[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "x_columns = data.columns.drop('attack_cat')\n",
    "x = data[x_columns].values\n",
    "dummies = pd.get_dummies(data['attack_cat'])  # Classification\n",
    "outcomes = dummies.columns\n",
    "num_classes = len(outcomes)\n",
    "y = dummies.values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.15, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 15)                2955      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                480       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 3,855\n",
      "Trainable params: 3,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 219022 samples, validate on 38651 samples\n",
      "Epoch 1/50\n",
      "219022/219022 - 7s - loss: 0.6797 - val_loss: 0.5783\n",
      "Epoch 2/50\n",
      "219022/219022 - 6s - loss: 0.5591 - val_loss: 0.5509\n",
      "Epoch 3/50\n",
      "219022/219022 - 6s - loss: 0.5374 - val_loss: 0.5323\n",
      "Epoch 4/50\n",
      "219022/219022 - 6s - loss: 0.5222 - val_loss: 0.5188\n",
      "Epoch 5/50\n",
      "219022/219022 - 6s - loss: 0.5125 - val_loss: 0.5158\n",
      "Epoch 6/50\n",
      "219022/219022 - 6s - loss: 0.5060 - val_loss: 0.5081\n",
      "Epoch 7/50\n",
      "219022/219022 - 6s - loss: 0.5018 - val_loss: 0.5046\n",
      "Epoch 8/50\n",
      "219022/219022 - 6s - loss: 0.4983 - val_loss: 0.4955\n",
      "Epoch 9/50\n",
      "219022/219022 - 6s - loss: 0.4952 - val_loss: 0.4979\n",
      "Epoch 10/50\n",
      "219022/219022 - 6s - loss: 0.4925 - val_loss: 0.5023\n",
      "Epoch 11/50\n",
      "219022/219022 - 6s - loss: 0.4905 - val_loss: 0.4931\n",
      "Epoch 12/50\n",
      "219022/219022 - 6s - loss: 0.4893 - val_loss: 0.4900\n",
      "Epoch 13/50\n",
      "219022/219022 - 6s - loss: 0.4870 - val_loss: 0.4886\n",
      "Epoch 14/50\n",
      "219022/219022 - 6s - loss: 0.4858 - val_loss: 0.4869\n",
      "Epoch 15/50\n",
      "219022/219022 - 6s - loss: 0.4842 - val_loss: 0.4874\n",
      "Epoch 16/50\n",
      "219022/219022 - 6s - loss: 0.4828 - val_loss: 0.4844\n",
      "Epoch 17/50\n",
      "219022/219022 - 6s - loss: 0.4818 - val_loss: 0.4936\n",
      "Epoch 18/50\n",
      "219022/219022 - 6s - loss: 0.4810 - val_loss: 0.4899\n",
      "Epoch 19/50\n",
      "219022/219022 - 6s - loss: 0.4795 - val_loss: 0.4826\n",
      "Epoch 20/50\n",
      "219022/219022 - 6s - loss: 0.4787 - val_loss: 0.4838\n",
      "Epoch 21/50\n",
      "219022/219022 - 6s - loss: 0.4779 - val_loss: 0.4830\n",
      "Epoch 22/50\n",
      "219022/219022 - 6s - loss: 0.4774 - val_loss: 0.4838\n",
      "Epoch 23/50\n",
      "219022/219022 - 6s - loss: 0.4766 - val_loss: 0.4810\n",
      "Epoch 24/50\n",
      "219022/219022 - 6s - loss: 0.4764 - val_loss: 0.4827\n",
      "Epoch 25/50\n",
      "219022/219022 - 6s - loss: 0.4751 - val_loss: 0.4868\n",
      "Epoch 26/50\n",
      "219022/219022 - 6s - loss: 0.4748 - val_loss: 0.4793\n",
      "Epoch 27/50\n",
      "219022/219022 - 6s - loss: 0.4742 - val_loss: 0.4791\n",
      "Epoch 28/50\n",
      "219022/219022 - 6s - loss: 0.4737 - val_loss: 0.4802\n",
      "Epoch 29/50\n",
      "219022/219022 - 6s - loss: 0.4726 - val_loss: 0.4791\n",
      "Epoch 30/50\n",
      "219022/219022 - 6s - loss: 0.4736 - val_loss: 0.4814\n",
      "Epoch 31/50\n",
      "219022/219022 - 6s - loss: 0.4724 - val_loss: 0.4766\n",
      "Epoch 32/50\n",
      "219022/219022 - 6s - loss: 0.4722 - val_loss: 0.4790\n",
      "Epoch 33/50\n",
      "219022/219022 - 6s - loss: 0.4718 - val_loss: 0.4841\n",
      "Epoch 34/50\n",
      "219022/219022 - 6s - loss: 0.4708 - val_loss: 0.4802\n",
      "Epoch 35/50\n",
      "219022/219022 - 6s - loss: 0.4696 - val_loss: 0.4756\n",
      "Epoch 36/50\n",
      "219022/219022 - 6s - loss: 0.4699 - val_loss: 0.4840\n",
      "Epoch 37/50\n",
      "219022/219022 - 6s - loss: 0.4690 - val_loss: 0.4764\n",
      "Epoch 38/50\n",
      "219022/219022 - 6s - loss: 0.4686 - val_loss: 0.4767\n",
      "Epoch 39/50\n",
      "219022/219022 - 6s - loss: 0.4682 - val_loss: 0.4768\n",
      "Epoch 40/50\n",
      "219022/219022 - 6s - loss: 0.4674 - val_loss: 0.4748\n",
      "Epoch 00040: early stopping\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1fad98b0348>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(30, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train, y_train, validation_data=(x_test,y_test),callbacks=[monitor], verbose=2, epochs=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Validation score: 0.8057230084603244\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}